{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./BP-MDD.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handling the target variable\n",
    "y = df[45]\n",
    "# y.replace({1: 0, 2: 1}, inplace=True)\n",
    "print(y.head)\n",
    "# making the classification in a binary classification\n",
    "df.drop(columns=[45], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = df.iloc[:,:5]\n",
    "train_index = [2,3,5,6]\n",
    "feat.iloc[train_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function with parameter\n",
    "# clf = classifier\n",
    "# X = features\n",
    "# y = target variable\n",
    "\n",
    "# Returns\n",
    "# specificity = array of specificity values\n",
    "# sensitivity = array of sensitivity values\n",
    "# acc = array of accuracies\n",
    "# precision = array of precisions\n",
    "# f1score = array of f1 scores\n",
    "# auc = area under curve\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import statistics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "def evaluate_model(clf, X, y):\n",
    "    specificity = list()\n",
    "    sensitvity = list()\n",
    "    f1_scores = list()\n",
    "    auc_scores = list()\n",
    "    accuracy_scores = list()\n",
    "    precision_scores = list()\n",
    "\n",
    "    # Standard Deviation\n",
    "    specificity_std = list()\n",
    "    sensitvity_std = list()\n",
    "    f1_scores_std = list()\n",
    "    auc_scores_std = list()\n",
    "    accuracy_scores_std = list()\n",
    "    precision_scores_std = list()\n",
    "\n",
    "    for i in range(0, 44):\n",
    "    # for i in range(0, 3):\n",
    "        # We generate the set of features\n",
    "        feat = X.iloc[:,:i+1]\n",
    "        target = y\n",
    "        fold = RepeatedKFold(n_splits=5, n_repeats=100, random_state=10)\n",
    "        # list for each features\n",
    "        split_sensi = list()\n",
    "        split_speci = list()\n",
    "        split_accu = list()\n",
    "        split_f1 = list()\n",
    "        split_precision = list()\n",
    "        split_auc = list()\n",
    "\n",
    "        for train_index, test_index in fold.split(feat):\n",
    "            X_train= feat.iloc[train_index,:]\n",
    "            X_test = feat.iloc[test_index,:]\n",
    "            y_train = target.iloc[train_index]\n",
    "            y_test= target.iloc[test_index]\n",
    "\n",
    "            # Fitting the classifier \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            # We have confusion matrx\n",
    "            cm = confusion_matrix(y_test,y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "            # Calucating metrics for each iteration\n",
    "            split_sensi.append(tp / (tp+fn))\n",
    "            split_speci.append(tn / (tn+fp))\n",
    "            \n",
    "            # Calculating accuracy\n",
    "            split_accu.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "            # Calculating auc_scores\n",
    "            split_auc.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "            # Calculating f1 scores\n",
    "            split_f1.append(f1_score(y_test, y_pred))\n",
    "\n",
    "            # Calculating precision\n",
    "            split_precision.append((tp / (tp + fp)) )\n",
    "        \n",
    "        # After competing all folds, appending the average of metrics to the main matric list\n",
    "        sensitvity.append(sum(split_sensi) / len(split_sensi))\n",
    "        specificity.append(sum(split_speci) / len(split_sensi))\n",
    "        precision_scores.append(sum(split_precision) / len(split_sensi))\n",
    "        auc_scores.append(sum(split_auc) / len(split_sensi))\n",
    "        accuracy_scores.append(sum(split_accu) / len(split_sensi))\n",
    "        f1_scores.append(sum(split_f1) / len(split_sensi))\n",
    "\n",
    "        sensitvity_std.append(statistics.stdev(split_sensi))\n",
    "        specificity_std.append(statistics.stdev(split_speci))\n",
    "        precision_scores_std.append(statistics.stdev(split_precision))\n",
    "        auc_scores_std.append(statistics.stdev(split_auc))\n",
    "        accuracy_scores_std.append(statistics.stdev(split_accu))\n",
    "        f1_scores_std.append(statistics.stdev(split_f1))\n",
    "    return {\n",
    "        \"specificity\" : specificity,\n",
    "        \"sensitvity\" : sensitvity,\n",
    "        \"f1_scores\" : f1_scores,\n",
    "        \"auc_scores\" : auc_scores,\n",
    "        \"accuracy_scores\" : accuracy_scores,\n",
    "        \"precision_scores\" : precision_scores,\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gaussianScores = evaluate_model(GaussianNB(), df, y)\n",
    "print(gaussianScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting number of features vs the metrics\n",
    "\n",
    "# number of features vs accuracy\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Accuracy\": np.array(gaussianScores['accuracy_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Accuracy', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs specificity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Specificity\": np.array(gaussianScores['specificity'])})\n",
    "sns.lineplot(x='number_of_features', y='Specificity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs sensitvity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Sensitvity\": np.array(gaussianScores['sensitvity'])})\n",
    "sns.lineplot(x='number_of_features', y='Sensitvity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs f1_scores\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"F1_score\": np.array(gaussianScores['f1_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='F1_score', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs auc\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Auc\": np.array(gaussianScores['auc_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Auc', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs precision\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Precision_score\": np.array(gaussianScores['precision_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Precision_score', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For maximum values for each classifiers\n",
    "maxValues = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting index for the maximum values\n",
    "maxIndex = np.argmax(np.array(gaussianScores['accuracy_scores']))\n",
    "\n",
    "maxValues.append([\"Naive Bayes\", \n",
    "maxIndex,\n",
    "gaussianScores[\"accuracy_scores\"][maxIndex], \n",
    "gaussianScores[\"specificity\"][maxIndex],\n",
    "gaussianScores[\"sensitvity\"][maxIndex],\n",
    "gaussianScores[\"f1_scores\"][maxIndex],\n",
    "gaussianScores[\"auc_scores\"][maxIndex],\n",
    "gaussianScores[\"precision_scores\"][maxIndex],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Decision Tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionScores = evaluate_model(DecisionTreeClassifier(), df, y)\n",
    "print(decisionScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting number of features vs the metrics\n",
    "\n",
    "# number of features vs accuracy\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Accuracy\": np.array(decisionScores['accuracy_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Accuracy', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs specificity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Specificity\": np.array(decisionScores['specificity'])})\n",
    "sns.lineplot(x='number_of_features', y='Specificity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs sensitvity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Sensitvity\": np.array(decisionScores['sensitvity'])})\n",
    "sns.lineplot(x='number_of_features', y='Sensitvity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs f1_scores\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"F1_score\": np.array(decisionScores['f1_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='F1_score', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs auc\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Auc\": np.array(decisionScores['auc_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Auc', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs precision\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Precision_score\": np.array(decisionScores['precision_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Precision_score', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting index for the maximum values\n",
    "maxIndex = np.argmax(np.array(decisionScores['accuracy_scores']))\n",
    "\n",
    "maxValues.append([\"Decision Tree\", \n",
    "maxIndex,\n",
    "decisionScores[\"accuracy_scores\"][maxIndex], \n",
    "decisionScores[\"specificity\"][maxIndex],\n",
    "decisionScores[\"sensitvity\"][maxIndex],\n",
    "decisionScores[\"f1_scores\"][maxIndex],\n",
    "decisionScores[\"auc_scores\"][maxIndex],\n",
    "decisionScores[\"precision_scores\"][maxIndex]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomForestScores = evaluate_model(RandomForestClassifier(n_jobs=-1, max_depth=5), df, y)\n",
    "print(randomForestScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting number of features vs the metrics\n",
    "\n",
    "# number of features vs accuracy\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Accuracy\": np.array(randomForestScores['accuracy_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Accuracy', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs specificity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Specificity\": np.array(randomForestScores['specificity'])})\n",
    "sns.lineplot(x='number_of_features', y='Specificity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs sensitvity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Sensitvity\": np.array(randomForestScores['sensitvity'])})\n",
    "sns.lineplot(x='number_of_features', y='Sensitvity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs f1_scores\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"F1_score\": np.array(randomForestScores['f1_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='F1_score', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs auc\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Auc\": np.array(randomForestScores['auc_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Auc', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs precision\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Precision_score\": np.array(randomForestScores['precision_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Precision_score', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting index for the maximum values\n",
    "maxIndex = np.argmax(np.array(randomForestScores['accuracy_scores']))\n",
    "\n",
    "maxValues.append([\"Random Forest\", randomForestScores[\"accuracy_scores\"][maxIndex], \n",
    "maxIndex,\n",
    "randomForestScores[\"specificity\"][maxIndex],\n",
    "randomForestScores[\"sensitvity\"][maxIndex],\n",
    "randomForestScores[\"f1_scores\"][maxIndex],\n",
    "randomForestScores[\"auc_scores\"][maxIndex],\n",
    "randomForestScores[\"precision_scores\"][maxIndex]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Linear Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "linearScores = evaluate_model(LinearDiscriminantAnalysis(), df, y)\n",
    "print(linearScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting number of features vs the metrics\n",
    "\n",
    "# number of features vs accuracy\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Accuracy\": np.array(linearScores['accuracy_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Accuracy', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs specificity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Specificity\": np.array(linearScores['specificity'])})\n",
    "sns.lineplot(x='number_of_features', y='Specificity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs sensitvity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Sensitvity\": np.array(linearScores['sensitvity'])})\n",
    "sns.lineplot(x='number_of_features', y='Sensitvity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs f1_scores\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"F1_score\": np.array(linearScores['f1_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='F1_score', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs auc\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Auc\": np.array(linearScores['auc_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Auc', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs precision\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Precision_score\": np.array(linearScores['precision_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Precision_score', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting index for the maximum values\n",
    "maxIndex = np.argmax(np.array(linearScores['accuracy_scores']))\n",
    "\n",
    "maxValues.append([\"LDA\", \n",
    "maxIndex,\n",
    "linearScores[\"accuracy_scores\"][maxIndex], \n",
    "linearScores[\"specificity\"][maxIndex],\n",
    "linearScores[\"sensitvity\"][maxIndex],\n",
    "linearScores[\"f1_scores\"][maxIndex],\n",
    "linearScores[\"auc_scores\"][maxIndex],\n",
    "linearScores[\"precision_scores\"][maxIndex]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discrimimant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Quadratic Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "quadraticScores = evaluate_model(QuadraticDiscriminantAnalysis(), df, y)\n",
    "print(quadraticScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features vs accuracy\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Accuracy\": np.array(quadraticScores['accuracy_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Accuracy', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs specificity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Specificity\": np.array(quadraticScores['specificity'])})\n",
    "sns.lineplot(x='number_of_features', y='Specificity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs sensitvity\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Sensitvity\": np.array(quadraticScores['sensitvity'])})\n",
    "sns.lineplot(x='number_of_features', y='Sensitvity', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs f1_scores\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"F1_score\": np.array(quadraticScores['f1_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='F1_score', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs auc\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Auc\": np.array(quadraticScores['auc_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Auc', data=data)\n",
    "plt.show()\n",
    "\n",
    "# number of features vs precision\n",
    "data = pd.DataFrame({\"number_of_features\" : np.array(range(1,45)),\n",
    "\"Precision_score\": np.array(quadraticScores['precision_scores'])})\n",
    "sns.lineplot(x='number_of_features', y='Precision_score', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting index for the maximum values\n",
    "maxIndex = np.argmax(np.array(quadraticScores['accuracy_scores']))\n",
    "\n",
    "maxValues.append([\"QDA\", \n",
    "maxIndex,\n",
    "quadraticScores[\"accuracy_scores\"][maxIndex], \n",
    "quadraticScores[\"specificity\"][maxIndex],\n",
    "quadraticScores[\"sensitvity\"][maxIndex],\n",
    "quadraticScores[\"f1_scores\"][maxIndex],\n",
    "quadraticScores[\"auc_scores\"][maxIndex],\n",
    "quadraticScores[\"precision_scores\"][maxIndex]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the maximum values\n",
    "print(maxValues)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf0502d1a5b9759e49df08dfa766bd2d6a64919df3bd5a34ee3df9e32f4e0ec2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('CS540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
